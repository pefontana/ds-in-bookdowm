[["index.html", "Data Science in Julia for Hackers Introduction Open source Prologue Table of contents", " Data Science in Julia for Hackers Lambdaclass 2021-04-22 Introduction Open source This book is currently in a beta version. We are looking forward to getting feedback and criticism: Submit a GitHub issue here. Mail us to martina.cantaro@lambdaclass.com Thank you! There are 2 ways in which you can read this book: 1. In our web page https://datasciencejuliahackers.com/ 1. If you prefer a dynamic version of the book, you can download the repo in GitHub and run the .jl files in a Pluto notebook The pdf version of the book will be available in a few weeks Bayes hacking Prologue This book is written by Federico Carrone, Herman Obst Demaestri and Mariano Nicolini. Thanks to Martina Cantaro, Camilo Plata, Manuel Puebla, Lucas Raúl Fernandez Piana, Osvaldo Martin, Iñaki Garay and Mariana Vinyolas. We have a habit in writing articles published in scientific journals to make the work as finished as possible, to cover up all the tracks, to not worry about the blind alleys or describe how you had the wrong idea first, and so on - Richard Feynman Only that what’s open can be true. Full transparency is best for me and you - OpenBSD One of the first things to note about this book is that it is not an academic textbook. The authors of this book are not academics but a multidisciplinary team of passionate, amateur practitioners from different backgrounds, namely Engineering, Computer Science, Physics and Economy, that found a common ground to write this book, and that day by day keep reading, learning and applying new approaches, technologies and ways of thinking. This book lies somewhere in between a methodological recipe and a theoretical intensive textbook. What we want to deliver is a mathematical and computational methodology to face concrete Data Science problems, that is, applying theory and science to real-world problems involving data. The relationship between theory and practice is complex. Considering them as a whole can take us much farther. These pages may offer the theorist a way to think about problematic situations in a more down to earth manner, and to the practitioner, stimulation to go beyond the mere application of programming libraries and tools. As the name of the book states, this is a book for Hackers. The term can have opposite connotations, depending on who is pronouncing it and to whom it refers to. In media and pop culture, it is associated with cyber-criminals, people that use computers and technology with malicious intent. In the cyber-security domain, hackers are, as stated in the final chapter of Hacking: The Art of Exploitation by Jon Erickson, “…just people with innovative spirits and an in-depth knowledge of technology.” But the definition we like the most is borrowed from The Jargon File’s glossary, written by Eric S. Raymond, &gt; “A person who delights in having an intimate understanding of the internal workings of a system, computers and computer networks in particular.” It is in this sense that this book is meant for hackers: it will lead you down a road with a results-driven perspective, slowly growing intuition about the inner workings of many problems involving data and what they all have in common, with an emphasis on application. The name of the book is also inspired by the great Bayesian Methods for Hackers, which had a big influence on the topics and the approach of this book. The situations presented in the book are diverse, reflecting the broad spectrum of application of Data Science. The solutions, however, don’t try to be beautiful or perfect. We know the path to resolving real-life problems is muddy, that sometimes you might feel lost and that your ideas may change as you try to find the solution, but we encourage to embrace these coarse features and accept the sharp corners in this learning-through-solving journey. Suggestions, criticism and feedback are very appreciated, this book is alive and will evolve as a consequence of its environment’s response. Although this book isn’t intended to have obscure, overly technical or academic definitions that may put the reader off, people with a little programming knowledge, a basic understanding of Calculus and some modelling intuition will get the most out of these pages. Familiarity with a high-level language like Python, taking derivatives and some function analysis should be enough for the reader to follow through with the book. All chapters include the code for the reader to play, explore and implement the solutions on their own. Moreover, links to notebooks with the worked solutions are available. Why Data Science in Julia? There are several reasons why Julia is great language for Data Science. Julia code is readable, specially in regards to math-related computations. Julia is designed for math from the ground up, and considering the great amounts of math involved in Data Science, this makes it a very convenient programming language to work with. Another interesting aspect of Julia is its syncretism. This is due to its historical background: it was developed by a team of scientists who wanted a tool to address problems they typically encountered while doing research. Often, scientists are faced with complicated situations that need to integrate various tools and find an equilibrium point among many requirements, like learning to program in a new language and to efficiently use the equipment and hardware available. Julia is the perfect tool for these needs, suited for implementing high-level solutions than can be tested easily and quickly in an interactive manner, and allowing to write highly performant code in a simple way when needed, all in the same language. Moreover, features like multiple-dispatch and simple syntax, allow for great composability between packages as well as scalability, making the task of writing software much more user friendly and maintainable. Table of contents Part I: Data Science and Julia First chapter: Science, technology, models and epistemology. Second chapter: Introduction to the Julia programming language, showing examples of code and some first steps. Part II: Bayesian Statistics Third chapter: An introduction to probability, probability distributions and Bayes’ interpretation. Fourth chapter: Using a Naive-Bayes approach we construct a simple spam email filter. Fifth chapter: An introduction to Probabilistic Programming and some simple examples using the Turing.jl package. Sixth chapter: We estimate the gravity of Mars to compute the escape velocity, throwing stones and taking very simple measurements from it. Seventh chapter: We use a hierarchical bayesian model to estimate latent variables that describe Premier League´s football teams. Eighth chapter: We analyze how the scoring probability is affected by some variables, such as the distance from the hoop and the angle of shooting. Ninth chapter: We solve a problem of optimal pricing optimization using a bayesian point of view. Part III: Machine Learning Work in progress Part IV: Deep Learning Tenth chapter: Overview of Machine Learning and implementation of a simple convolutional neural network that is able to discriminate between pictures of bees and wasps. Part V: Scientific Machine Learning Eleventh chapter: We explain the Ultima Online Catastrophe using differential equations to build a population dynamics model. Twelfth chapter: A continuation of the Ultima Online Catastrophe, introducing the Universal Differential Equations to recover missing parts of scientific models. Part VI: Time Series and Forecasting Thirteenth chapter: We lay the foundations for time series analysis, focusing on the exponential smoothing method. "],["science-technology-and-epistemology.html", "Chapter 1 Science technology and epistemology 1.1 The difference between Science and Technology 1.2 What is technology? 1.3 References", " Chapter 1 Science technology and epistemology 1.1 The difference between Science and Technology Anyone would agree that science and technology have a lot in common, but what are the essential differences between them? And how do they interact with each other? These are fundamental questions for us, and ones that most people don’t have a clear answer for. This leads to misconceptions about key aspects on, for example, how knowledge is created and, more generally, how the world operates. One fundamental difference between the two is that while science tries to understand the physical world in which we live, technology aims to transform it. So, technology has a strong practical goal and science a more intellectual one. A very frequent misunderstanding that is nowadays very much encouraged by academia is to think that first, one needs to acquire all of the theoretical and formal knowledge about a subject matter before being able to apply it and transform reality. But in reality this is not the case. The process occurs, in most cases, in the opposite way: one lives in reality, interacts with it, takes action, makes mistakes, tries again, and in that way slowly discovers how the world works. And then, of course, it is useful to formalize our knowledge in order to be able to transmit that learning, lay solid foundations and to continue advancing in our understanding. 1.2 What is technology? Technology is the practice that allows us humans to transform reality. We have been doing it since the very beginning of the human era, and it is really what makes us different from other animals. Technology enables us to expand our capabilities. From taking a long, pointy stick to reach fruit high up in a tree, to transforming an entire landscape by building a huge electric dam, to the creation of artificial intelligence that helps us solve our deepest puzzles. With technology we transform our experience in the world. It is evident that the more we know about the reality in which we live, the more we will be able to modify it. But from this idea comes a very important question: what is the order in which innovation takes place? Is it necessary first to have an absolute understanding of the process we are trying to modify? Or is technology created in a more chaotic process, through trial and error? Does a child need to know the gear mechanisms of a bicycle to learn to ride it? Does Lionel Messi need to know about fluid dynamics to make the ball take a curved trajectory? Did the Romans need to know the Navier-Stokes equation in order to build their huge aqueducts? Knowledge is often acquired through experimentation, implementation and heuristics, in a process that involves more trial and error and less theoretical knowledge than many believe. And technological innovation tends to follows the same mechanism. Many breakthroughs were made by people who were taking risks, exploring, and stumbling in the dark with a destination in mind but no clear directions to get there, with theories only taking their definitive form once they arrived to a solution and were able to look back on what they had discovered. The current understanding of how knowledge is produced does not reflect how it actually happens in many cases. Having a better understanding of this process can help us do it better. Many people think of the innovation process as a more or less linear path that begins with theoretical discoveries, and only after formalization is achieved, practical uses are invented. This type of thinking is counterproductive because, being so widespread, it causes many people to be more concerned with constantly acquiring theoretical knowledge, rather than taking action and immersing themselves in practice. Fear of failure, of not getting it exactly right on the first try, also plays a role, perhaps encouraged by the way we tell stories about innovation. By looking at the history of technology and innovation, and who writes it, we see that the people who made the discoveries are rarely the ones who write the books about them. As Nassim Nicholas Taleb said in “The History Written by the Losers,” the people that are doing stuff don’t have time for writing. And perhaps because the non-practitioners are the ones who write about the findings of others, as time goes by, society ends up being convinced that there was indeed an arduous intellectual and academic work first, and then came its implementation. That –apparently– common sense in which knowledge is built from a purely intellectual work that can be done in the armchair at home, and that only after acquiring this sacred theoretical knowledge it is possible to come up with technology or innovation, is the one we need to question. This confusion in the order in which technological advances occur is seen constantly and in the most varied areas of the history of innovation. Take, for example, the development of the jet engine. It really had nothing to do with the discoveries of physicists researching, but with the cleverness and practical heuristics based on trial and error that engineers had developed, although in academic books it is stated the other way around. Or in a really practical field, finances. For years, senior traders that have been deploying several heuristics to make their trades, build portfolios that are much more complex and better performing than the ones generated by the pricing formulas that academics came up with and often didn’t stand the test of time. And this happened because, in the process of trying to generalize these heuristics into formal equations, the academics are constantly introducing fragility. That is, in the process of finding the laws that rule those dynamic systems, lot of cases are ignored, something that doesn’t happen to experienced traders. What cements the gap between theory and practice, is that finance PhDs then fail to understand how traders can correctly assign prices to financial derivatives without being familiar with a corpus of theorems that, to them, are indispensable to understand market dynamics. In this book we will try to take you, the reader, through a journey that is more similar to the real way in which knowledge is built: an iterative, hands-on process of problem-solving that gradually builds intuitions about how things work and why, that we can later formalize. 1.3 References Antifragile: Things That Gain from Disorder, ch 15 - Nassim Nicholas Taleb Infinite Powers: How Calculus Reveals the Secrets of the Universe - Steven Strogatz Lost in Math: How Beauty Leads Physics Astray - Sabine Hossenfelder The Joy of X Kolmogorov - Mathematics: It’s contents, method and meaning Freeman Dyson - Where Do the Laws of Nature Come From? Roger Penrose - Is Mathematics Invented or Discovered? How to tell science from pseudoscience Sabine Hossenfelder - Why the ‘Unreasonable Effectiveness’ of Mathematics? http://www.paulgraham.com/hp.html https://en.wikipedia.org/wiki/Apophatic_theology https://en.wikipedia.org/wiki/Falsifiability https://norvig.com/fact-check.html https://www.wired.com/2008/06/pb-theory/ https://fs.blog/2016/01/karl-popper-on-science-pseudoscience/ "],["meeting-julia.html", "Chapter 2 Meeting Julia 2.1 Why Julia 2.2 Julia presentation 2.3 Installation 2.4 First steps into the Julia world 2.5 Julia’s Ecosystem: Basic plotting and manipulation of DataFrames 2.6 Summary 2.7 References 2.8 Give us feedback", " Chapter 2 Meeting Julia 2.1 Why Julia People have asked us why we wrote this book using Julia instead of Python or R, which are the current standards in the data science world. While Python and R are also great choices, Julia is an up and coming language that will surely have an impact in the coming years. It performs faster than pure R and Python (as fast as C) while maintaining the same degree of readability, allowing us to write highly performant code in a simple way. Julia is already being used in many top-tier tech companies and scientific research —there are plenty of scientists and engineers of different disciplines collaborating with Julia, which gives us a wide range of possibilities to approach different problems. Often, libraries in languages like Python or R are optimized to be performant, but this usually involves writing code in other languages better suited for this task such as C or Fortran, as well as writing code to manage the communication between the high level language and the low level one. Julia, on the other hand, expands the possibilities of people who have concrete problems that involve a lot of computation. Libraries can be developed to be performant in plain Julia code, following some basic coding guidelines to get the most out of it. This enables useful libraries to be created by people without programming or Computer Science expertise. 2.2 Julia presentation Julia is a free and open-source general-purpose language, designed and developed by Jeff Bezanson, Alan Edelman, Viral B. Shah and Stefan Karpinski at MIT. Julia is created from scratch to be both fast and easy to understand, even for people who are not programmers or computer scientists. It has abstraction capabilities of high-level languages, while also being really fast, as its slogan calls “Julia looks like Python, feels like Lisp, runs like Fortran.” Before Julia, programming languages were limited to either having a simple syntax and good abstraction capabilities and therefore user-friendly or being high-performance, which was necessary to solve resource-intensive computations. This led applied scientists to face the task of not only learning two different languages, but also learning how to have them communicating with one another. This difficulty is called the two-language problem, which Julia creators aim to solve with this new language. Julia is dynamically typed and great for interactive use. It also uses multiple dispatch as a core design concept, which adds to the composability of the language. In conventional, single-dispatched programming languages, when invoking a method, one of the arguments has a special treatment since it determines which of the methods contained in a function is going to be applied. Multiple dispatch is a generalization of this for all the arguments of the function, so the method applied is going to be the one that matches exactly the number of types of the function call. 2.3 Installation For the installation process, we recommend you follow the instructions provided by the Julia team: &gt; Platform Specific Instructions for Official Binaries: These instructions will get you through a fresh installation of Julia depending on the specifications of your computer. It is a bare bones installation, so it will only include the basic Julia packages. All along the book, we are going to use specific Julia packages that you have to install before calling them in your code. Julia has a built-in packet manager that makes the task of installing new packages and checking compatibilities very easy. First, you will need to start a Julia session. For this, type in your terminal ~ julia julia&gt; At this point, your Julia session will have started. What you see right now is a Julia REPL (read-eval-print loop), an interactive command line prompt. Here you can quickly evaluate Julia expressions, get help about different Julia functionalities and much more. The REPL has a set of different modes you can activate with different keybindings. The Julian mode is the default one, where you can directly type any Julia expression and press the Enter key to evaluate and print it. The help mode is activated with an interrogation sign ‘?’ . You will notice that the prompt will now change, julia&gt; ? help?&gt; By typing the name of a function or a Julia package, you will get information about it as well as usage examples. Another available mode is the shell mode. This is just a way to input terminal commands in your Julia REPL. You can access this mode by typing a semicolon, julia&gt; ; shell&gt; Maybe one of the most used, along with the default Julian mode, is the package manager mode. When in this mode, you can perform tasks such as adding and updating packages. It is also useful to manage project environments and controlling package versions. To switch to the package manager, type a closing square bracket ‘],’ julia&gt; ] (@v1.5) pkg&gt; If you see the word ‘pkg’ in the prompt, it means you accessed the package manager successfully. To add a new package, you just need to write (@v1.5) pkg&gt; add NewPackage It is as simple as that! All Julia commands are case-sensitive, so be sure to write the package name –and in the future, all functions and variables too– correctly. 2.4 First steps into the Julia world As with every programming language, it is useful to know some of the basic operations and functionalities. We encourage you to open a Julia session REPL and start experimenting with all the code written in this chapter to start developing an intuition about the things that make Julia code special. The common arithmetical and logical operations are all available in Julia: \\(+\\): Add operator \\(-\\): Subtract operator \\(*\\): Product operator \\(/\\): Division operator Julia code is intended to be very similar to math. So instead of doing something like julia&gt; 2*x you can simply do julia&gt; 2x For this same purpose, Julia has a great variety of unicode characters, which enable us to write things like Greek letters and subscripts/superscripts, making our code much more beautiful and easy to read in a mathematical form. In general, unicode characters are activated by using ’‘, followed by the name of the character and then pressing the ’tab’ key. For example, julia&gt; \\beta # and next we press tab julia&gt; β You can add subscripts by using ‘_’ and superscripts by using ‘^,’ followed by the character(s) you want to modify and then pressing Tab. For example, julia&gt; L\\_0 # and next we press tab julia&gt; L₀ Unicodes behave just like any other letter of your keyboard. You can use them inside strings or as variable names and assign them a value. julia&gt; β = 5 5 julia&gt; \\&quot;The ⌀ of the circle is $β \\&quot; \\&quot;The ⌀ of the circle is 5 \\&quot; Some popular Greek letters already have their values assigned. julia&gt; \\pi # and next we press tab julia&gt; π π = 3.1415926535897... julia&gt; \\euler # and next we press tab julia&gt; ℯ ℯ = 2.7182818284590... You can see all the unicodes supported by Julia here The basic number types are also supported in Julia. We can explore this with the function typeof(), which outputs the type of its argument, as it is represented in Julia. Let’s see some examples, julia&gt;typeof(2) Int64 julia&gt;typeof(2.0) Float64 julia&gt;typeof(3 + 5im) Complex{Int64} These were examples of integers, floats and complex numbers. All data types in Julia start with a capital letter. Notice that if you want to do something like, julia&gt; 10/2 5.0 the output is a floating point number, although the two numbers in the operation are integers. This is because in Julia, division of integers always results in floats. When valid, you can always do julia&gt; Int64(5.0) 5 to convert from one data type to another. Following with the basics, let’s take a look at how logical or boolean operations are done in Julia. Booleans are written as ‘true’ and ‘false.’ The most important boolean operations for our purposes are the following: !: \\&quot;not\\&quot; logical operator &amp;: \\&quot;and\\&quot; logical operator |: \\&quot;or\\&quot; logical operator ==: \\&quot;equal to\\&quot; logical operator !=: \\&quot;different to\\&quot; logical operator &gt;: \\&quot;greater than\\&quot; operator &lt;: \\&quot;less than\\&quot; operator &gt;=: \\&quot;greater or equal to\\&quot; operator &lt;=: \\&quot;less or equal to\\&quot; operator Some examples of these, julia&gt; true &amp; true true julia&gt; true &amp; false false julia&gt; true &amp; !false true julia&gt; 3 == 3 true julia&gt; 4 == 5 false julia&gt; 7 &lt;= 7 true Comparisons can be chained to have a simpler mathematical readability, like so: julia&gt; 10 &lt;= 11 &lt; 24 true julia&gt; 5 &gt; 2 &lt; 1 false The next important topic in this Julia programming basics, is the strings data type and basic manipulations. As in many other programming languages, strings are created between ‘\",’ julia&gt; \\&quot;This is a Julia string!\\&quot; \\&quot;This is a Julia string!\\&quot; You can access a particular character of a string by writing the index of that character in the string between brackets right next to it. Likewise, you can access a substring by writing the first and the last index of the substring you want, separated by a colon, all this between brackets. This is called slicing, and it will be very useful later when working with arrays. Here’s an example: julia&gt; \\&quot;This is a Julia string!\\&quot;[1] # this will output the first character of the string and other related information. &#39;T&#39;: ASCII/Unicode U+0054 (category Lu: Letter, uppercase) julia&gt; \\&quot;This is a Julia string!\\&quot;[1:4] # this will output the substring obtained of going from the first index to the fourth \\&quot;This\\&quot; A really useful tool when using strings is string interpolation. This is a way to evaluate an expression inside a string and print it. This is usually done by writing a dollar symbol $ $ $ followed by the expression between parentheses. For example, julia&gt; \\&quot;The product between 4 and 5 is $(4 * 5)\\&quot; \\&quot;The product between 4 and 5 is 20\\&quot; This wouldn’t be a programming introduction if we didn’t include printing ‘Hello World!’ , right? Printing in Julia is very easy. There are two functions for printing: print() and println(). The former will print the string you pass in the argument, without creating a new line. What this means is that, for example, if you are in a Julia REPL and you call the print() function two or more times, the printed strings will be concatenated in the same line, while successive calls to the println() function will print every new string in a new, separated line from the previous one. So, to show this we will need to execute two print actions in one console line. To execute multiple actions in one line you just need to separate them with a ;. julia&gt; print(&quot;Hello&quot;); print(&quot; world!&quot;) Hello world! julia&gt; println(&quot;Hello&quot;); println(&quot;world!&quot;) Hello world! It’s time now to start introducing collections of data in Julia. We will start by talking about arrays. As in many other programming languages, arrays in Julia can be created by listing objects between square brackets separated by commas. For example, julia&gt; int_array = [1, 2, 3] 3-element Array{Int64,1}: 1 2 3 julia&gt; str_array = [\\&quot;Hello\\&quot;, \\&quot;World\\&quot;] 2-element Array{String,1}: \\&quot;Hello\\&quot; \\&quot;World\\&quot; As you can see, arrays can store any type of data. If all the data in the array is of the same type, it will be compiled as an array of that data type. You can see that in the pattern that the Julia REPL prints out: Firstly, it displays how many elements there are in the collection. In our case, 3 elements in int_array and 2 elements in str_array. When dealing with higher dimensionality arrays, the shape will be informed. Secondly, the output shows the type and dimensionality of the array. The first element inside the curly brackets specifies the type of every member of the array, if they are all the same. If this is not the case, type ‘Any’ will appear, meaning that the collection of objects inside the array is not homogeneous in its type. Compilation of Julia code tends to be faster when arrays have a defined type, so it is recommended to use homogeneous types when possible. The second element inside the curly braces tells us how many dimensions there arein the array. Our example shows two one-dimensional arrays, hence a 1 is printed. Later, we will introduce matrices and, naturally, a 2 will appear in this place instead a 1. Finally, the content of the array is printed in a columnar way. When building Julia, the convention has been set so that it has column-major ordering. So you can think of standard one-dimensional arrays as column vectors, and in fact this will be mandatory when doing calculations between vectors or matrices. A row vector (or a \\(1\\)x\\(n\\) array), in the other hand, can be defined using whitespaces instead of commas, julia&gt; [3 2 1 4] 1×4 Array{Int64,2}: 3 2 1 4 In contrast to other languages, where matrices are expressed as ‘arrays of arrays,’ in Julia we write the numbers in succession separated by whitespaces, and we use a semicolon to indicate the end of the row, just like we saw in the example of a row vector. For example, julia&gt; [1 1 2; 4 1 0; 3 3 1] 3×3 Array{Int64,2}: 1 1 2 4 1 0 3 3 1 The length and shape of arrays can be obtained using the length() and size() functions respectively. julia&gt; length([1, -1, 2, 0]) 4 julia&gt; size([1 0; 0 1]) (2, 2) julia&gt; size([1 0; 0 1], 2) # you can also specify the dimension where you want the shape to be computed 2 An interesting feature in Julia is broadcasting. Suppose you wanted to add the number 2 to every element of an array. You might be tempted to do julia&gt; 2 + [1, 1, 1] ERROR: MethodError: no method matching +(::Array{Int64,1}, ::Int64) For element-wise addition, use broadcasting with dot syntax: array .+ scalar Closest candidates are: +(::Any, ::Any, ::Any, ::Any...) at operators.jl:538 +(::Complex{Bool}, ::Real) at complex.jl:301 +(::Missing, ::Number) at missing.jl:115 ... Stacktrace: [1] top-level scope at REPL[18]:1 As you can see, the expression returns an error. If you watch this error message closely, it gives you a good suggestion about what to do. If we now try writing a period ‘.’ right before the plus sign, we get julia&gt; 2 .+ [1, 1, 1] 3-element Array{Int64,1}: 3 3 3 What we did was broadcast the sum operator ‘+’ over the entire array. This is done by adding a period before the operator we want to broadcast. In this way we can write complicated expressions in a much cleaner, simpler and compact way. This can be done with any of the operators we have already seen, julia&gt; 3 .&gt; [2, 4, 5] # this will output a bit array with 0s as false and 1s as true 3-element BitArray{1}: 1 0 0 If we do a broadcasting operation between two arrays with the same shape, whatever operation you are broadcasting will be done element-wise. For example, julia&gt; [7, 2, 1] .* [10, 4, 8] 3-element Array{Int64,1}: 70 8 8 julia&gt; [10 2 35] ./ [5 2 7] 1×3 Array{Float64,2}: 2.0 1.0 5.0 julia&gt; [5 2; 1 4] .- [2 1; 2 3] 2×2 Array{Int64,2}: 3 1 -1 1 If we use the broadcast operator between a column vector and a row vector instead, the broadcast is done for every row of the first vector and every column of the second vector, returning a matrix, julia&gt; [1, 0, 1] .+ [3 1 4] 3×3 Array{Int64,2}: 4 2 5 3 1 4 4 2 5 Another useful tool when dealing with arrays are concatenations. Given two arrays, you can concatenate them horizontally or vertically. This is best seen in an example julia&gt; vcat([1, 2, 3], [4, 5, 6]) # this concatenates the two arrays vertically, giving us a new long array 6-element Array{Int64,1}: 1 2 3 4 5 6 julia&gt; hcat([1, 2, 3], [4, 5, 6]) # this stacks the two arrays one next to the other, returning a matrix 3×2 Array{Int64,2}: 1 4 2 5 3 6 With some of these basic tools to start getting your hands dirty in Julia, we can get going into some other functionalities like loops and function definitions. We have already seen a for loop. For loops are started with a for keyword, followed by the name of the iterator and the range of iterations we want our loop to cover. Below this for statement we write what we want to be performed in each loop and we finish with an end keyword statement. Let’s return to the example we made earlier, julia&gt; for i in 1:100 println(i) end The syntax 1:100 is the Julian way to define a range of all the numbers from 1 to 100, with a step of 1. We could have set 1:2:100 if we wanted to jump between numbers with a step size of 2. We can also iterate over collections of data, like arrays. Consider the next block of code where we define an array and then iterate over it, julia&gt; arr = [1, 3, 2, 2] julia&gt; for element in arr println(element) end 1 3 2 2 As you can see, the loop was done for each element of the array. It might be convenient sometimes to iterate over a collection. Conditional statements in Julia are very similar to most languages. Essentially, a conditional statement starts with the if keyword, followed by the condition that must be evaluated to true or false, and then the body of the action to apply if the condition evaluates to true. Then, optional elseif keywords may be used to check for additional conditions, and an optional else keyword at the end to execute a piece of code if all of the conditions above evaluate to false. Finally, as usual in Julia, the conditional statement block finishes with an end keyword. julia&gt; x = 3 julia&gt; if x &gt; 2 println(\\&quot;x is greater than 2\\&quot;) elseif 1 &lt; x &lt; 2 println(\\&quot;x is in between 1 and 2\\&quot;) else println(\\&quot;x is less than 1\\&quot;) end x is greater than 2 Now consider the code block below, where we define a function to calculate a certain number of steps of the Fibonacci sequence, julia&gt; n1 = 0 julia&gt; n2 = 1 julia&gt; m = 10 julia&gt; function fibonacci(n1, n2, m) fib = Array{Int64,1}(undef, m) fib[1] = n1 fib[2] = n2 for i in 3:m fib[i] = fib[i-1] + fib[i-2] end return fib end fibonacci (generic function with 1 method) Here, we first made some variable assignments, variables \\(n1\\), \\(n2\\) and \\(m\\) were assigned values 0, 1 and 10. Variables are assigned simply by writing the name of the variable followed by an ‘equal’ sign, and followed finally by the value you want to store in that variable. There is no need to declare the data type of the value you are going to store. Then, we defined the function body for the fibonacci series computation. Function blocks start with the function keyword, followed by the name of the function and the arguments between brackets, all separated by commas. In this function, the arguments will be the first two numbers of the sequence and the total length of the fibonacci sequence. Inside the body of the function, everything is indented. Although this is not strictly necessary for the code to run, it is a good practice to have from the bbeginning, since we want our code to be readable. At first, we initialize an array of integers of one dimension and length \\(m\\), by allocating memory. This way of initializing an array is not strictly necessary, you could have initialized an empty array and start filling it later in the code. But it is definitely a good practice to learn for a situation like this, where we know how long our array is going to be and optimizing code performance in Julia. The memory allocation for this array is done by initializing the array as we have already seen earlier. julia {Int64,1}just means we want a one-dimensional array of integers. The new part is the one between parenthesis, julia (undef, m). This just means we are initializing the array with undefined values –which will be later modified by us–, and that there will be a number \\(m\\) of them. Don’t worry too much if you don’t understand all this right now, though. We then proceed to assign the two first elements of the sequence and calculate the rest with a for loop. Finally, an end keyword is necessary at the end of the for loop and another one to end the definition of the function. Evaluating our function in the variables \\(n1\\), \\(n2\\) and \\(m\\) already defined, gives us: julia&gt; fibonacci(n1, n2, m) 10-element Array{Int64,1}: 0 1 1 2 3 5 8 13 21 34 Remember the broadcasting operation, that dot we added to the bbeginning of another operator to apply it on an entire collection of objects? It turns out that this can be done with functions as well! Consider the following function, julia&gt; function isPositive(x) if x &gt;= 0 return true elseif x &lt; 0 return false end end isPositive (generic function with 1 method) julia&gt; isPositive(3) true julia&gt; isPositive.([-1, 1, 3, -5]) 4-element BitArray{1}: 0 1 1 0 As you can see, we broadcasted the isPositive() function over every element of an array by adding a dot next to the end of the function name. It is as easy as that! Once you start using this feature, you will notice how useful it is. One thing concerning functions in Julia is the ‘bang’(!) convention. Functions that have a name ending with an exclamation mark (or bang), are functions that change their inputs in-place. Consider the example of the pop! function from the Julia Base package. Watch closely what happens to the array over which we apply the function. julia&gt; arr = [1, 2, 3] julia&gt; n = pop!(arr) 3 julia&gt; arr 2-element Array{Int64,1}: 1 2 julia&gt; n 3 Did you understand what happened? First, we defined an array. Then, we applied the pop!() function, which returns the last element of the array and assigns it to n. But notice that when we call our arr variable to see what it is storing, now the number 3 is gone. This is what functions with a bang do and what we mean with modifying in-place. Try to follow this convention whenever you define a function that will modify other objects in-place! Sometimes, you will be in a situation where you may need to use some function, but you don’t really need to give it name and store it, because it’s not very relevant to your code. For these kinds of situations, an anonymous or lambda function may be what you need. Typically, anonymous functions will be used as arguments to higher-order functions. This is just a fancy name to functions that accept other functions as arguments, that is what makes them of higher-order. We can create an anonymous function and apply it to each element of a collection by using the map() keyword. You can think of the map() function as a way to broadcast any function over a collection. Anonymous functions are created using the arrow -&gt; syntax. At the left-hand side of the arrow, you must specify what the arguments of the function will be and their name. At the right side of the arrow, you write the recipe of the things to do with these arguments. Let’s use an anonymous function to define a not-anonymous function, just to illustrate the point. julia&gt; f = (x,y) -&gt; x + y #1 (generic function with 1 method) julia&gt; f(2,3) 5 You can think about what we did as if \\(f\\) were a variable that is storing some function. Then, when calling \\(f(2,3)\\) Julia understands we want to evaluate the function it is storing with the values 2 and 3. Let’s see now how the higher-order function map() uses anonymous functions. We will broadcast our anonymous function x^2 + 5 over all the elements of an array. julia&gt; map(x -&gt; x^2 + 5, [2, 4, 6, 3, 3]) 5-element Array{Int64,1}: 9 21 41 14 14 The first argument of the map function is another function. You can define new functions and then use them inside map, but with the help of anonymous functions you can simply create a throw-away function inside map’s arguments. This function we pass as an argument, is then applied to every member of the array we input as the second argument. \"\"\" Now let’s introduce another data collection: Dictionaries. A dictionary is a collection of key-value pairs. You can think of them as arrays, but instead of being indexed by a sequence of numbers they are indexed by keys, each one linked to a value. To create a dictionary we use the function Dict() with the key-value pairs as arguments. Dict(key1 =&gt; value1, key2 =&gt; value2). julia&gt; Dict(&quot;A&quot; =&gt; 1, &quot;B&quot; =&gt; 2) Dict{String,Int64} with 2 entries: &quot;B&quot; =&gt; 2 &quot;A&quot; =&gt; 1 So we created our first dictionary. Let’s review what the Julia REPL prints out: Dict{String,Int64} tells us the dictionary data type that Julia automatically assigns to the pair (key,value). In this example, the keys will be strings and the values, integers. Finally, it prints all the (key =&gt; value) elements of the dictionary. In Julia, the keys and values of a dictionary can be of any type. julia&gt; Dict(&quot;x&quot; =&gt; 1.4, &quot;y&quot; =&gt; 5.3) Dict{String,Float64} with 2 entries: &quot;x&quot; =&gt; 1.4 &quot;y&quot; =&gt; 5.3 julia&gt; Dict(1 =&gt; 10.0, 2 =&gt; 100.0) Dict{Int64,Float64} with 2 entries: 2 =&gt; 100.0 1 =&gt; 10.0 Letting Julia automatically assign the data type can cause bugs or errors when adding new elements. Thus, it is a good practice to assign the data type of the dictionary ourselves. To do it, we just need to indicate it in between brackets { } after the Dict keyword: Dict{key type, value type}(key1 =&gt; value1, key2 =&gt; value2) julia&gt; Dict{Int64,String}(1 =&gt; &quot;Hello&quot;, 2 =&gt; &quot;Wormd&quot;) Dict{Int64,String} with 2 entries: 2 =&gt; &quot;Wormd&quot; 1 =&gt; &quot;Hello&quot; Now let’s see the dictionary’s basic functions. First, we will create a dictionary called “languages” that contains the names of programming languages as keys and their release year as values. julia&gt; languages = Dict{String,Int64}(&quot;Julia&quot; =&gt; 2012, &quot;Java&quot; =&gt; 1995, &quot;Python&quot; =&gt; 1990) Dict{String,Int64} with 3 entries: &quot;Julia&quot; =&gt; 2012 &quot;Python&quot; =&gt; 1990 &quot;Java&quot; =&gt; 1995 To grab a key’s value we need to indicate it in between brackets []. julia&gt; languages[&quot;Julia&quot;] 2012 We can easily add an element to the dictionary. julia&gt; languages[&quot;C++&quot;] = 1980 1980 julia&gt; languages Dict{String,Int64} with 4 entries: &quot;Julia&quot; =&gt; 2012 &quot;Python&quot; =&gt; 1990 &quot;Java&quot; =&gt; 1995 &quot;C++&quot; =&gt; 1980 We do something similar to modify a key’s value: julia&gt; languages[&quot;Python&quot;] = 1991 1991 julia&gt; languages Dict{String,Int64} with 3 entries: &quot;Julia&quot; =&gt; 2012 &quot;Python&quot; =&gt; 1991 &quot;C++&quot; =&gt; 1980 Notice that the ways of adding and modifying a value are identical. That is because keys of a dictionary can never be repeated or modified. Since each key is unique, assigning a new value for a key overrides the previous one. To delete an element we use the delete! method. julia&gt; delete!(languages,&quot;Java&quot;) Dict{String,Int64} with 3 entries: &quot;Julia&quot; =&gt; 2012 &quot;Python&quot; =&gt; 1990 &quot;C++&quot; =&gt; 1980 To finish, let’s see how to iterate over a dictionary. julia&gt; for(key,value) in languages println(&quot;$key was released in $value&quot;) end Julia was released in 2012 Python was released in 1991 C++ was released in 1980 \"\"\" Now that we have discussed the most important details of Julia’s syntax, let’s focus our attention on some of the packages in Julia’s ecosystem.\" 2.5 Julia’s Ecosystem: Basic plotting and manipulation of DataFrames Julia’s ecosystem is composed by a variety of libraries which focus on technical domains such as Data Science (DataFrames.jl, CSV.jl, JSON.jl), Machine Learning (MLJ.jl, Flux.jl, Turing.jl) and Scientific Computing (DifferentialEquations.jl), as well as more general purpose programming (HTTP.jl, Dash.jl). We will now consider one of the libraries that will be accompanying us throughout the book to make visualizations, Plots.jl. To install the Plots.jl library we need to go to the Julia package manager mode as we saw earlier. julia&gt; ] (@v1.5) pkg&gt; (@v1.5) pkg&gt; add Plots.jl There are some other great packages like Gadfly.jl and VegaLite.jl, but Plots will be the best to get you started. Let’s import the library with the ‘using’ keyword and start making some plots. We will plot the first ten numbers of the fibonacci sequence using the scatter() function. begin using Plots sequence = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] scatter(sequence, xlabel=&quot;n&quot;, ylabel=&quot;Fibonacci(n)&quot;, color=&quot;purple&quot;, label=false, size=(450, 300)) end 2.5.1 Plotting with Plots.jl Let’s make a plot of the 10 first numbers in the fibonacci sequence. For this, we can make use of the scatter() function: The only really important argument of the scatter function in the example above is sequence, the first one, which tells the function what is the data we want to plot. The other arguments are just details to make the visualization prettier. Here we have used the scatter function because we want a discrete plot for our sequence. In case we wanted a continuous one, we could have used plot(). Let’s see this applied to our fibonacci sequence: plot(sequence, xlabel=&quot;x&quot;, ylabel=&quot;Fibonacci&quot;, linewidth=3, label=false, color=&quot;green&quot;, size=(450, 300)) begin plot(sequence, xlabel=&quot;x&quot;, ylabel=&quot;Fibonacci&quot;, linewidth=3, label=false, color=&quot;green&quot;, size=(450, 300)) scatter!(sequence, label=false, color=&quot;purple&quot;, size=(450, 300)) end In the example above, a plot is created when we call the plot() function. What the scatter!() call then does, is to modify the global state of the plot in-place. If not done this way, both plots wouldn’t be sketched together. A nice feature that the Plots.jl package offers, is the fact of changing plotting backends. There exist various plotting packages in Julia, and each one has its own special features and aesthetic flavour. The Plots.jl package integrates these plotting libraries and acts as an interface to communicate with them in an easy way. By default, the GR backend is the one used. In fact, this was the plotting engine that generated the plots we have already done. The most used and maintained plotting backends up to date, are the already mentioned GR, Plotly/PlotlyJS, PyPlot, UnicodePlots and InspectDR. The backend you choose will depend on the particular situation you are facing. For a detailed explanation on backends, we recommend you visit the Julia Plots documentation. Through the book we will be focusing on the GRbackend, but as a demonstration of the ease of changing from one backend to another, consider the code below. The only thing added to the code for plotting that we have already used, is the pyplot() call to change the backend. If you have already coded in Python, you will feel familiar with this plotting backend. begin pyplot() plot(sequence, xlabel=&quot;x&quot;, ylabel=&quot;Fibonacci&quot;, linewidth=3, label=false, color=&quot;green&quot;, size=(450, 300)) scatter!(sequence, label=false, color=&quot;purple&quot;, size=(450, 300)) end Analogously, we can use the plotlyjs backend, which is specially suited for interactivity. begin plotlyjs() plot(sequence, xlabel=&quot;x&quot;, ylabel=&quot;Fibonacci&quot;, linewidth=3, label=false, color=&quot;green&quot;, size=(450, 300)) scatter!(sequence, label=false, color=&quot;purple&quot;, size=(450, 300)) end Each of these backends has its own scope, so there may be plots that one backend can do that other can’t. For example, 3D plots are not supported for all backends. The details are well explained in the Julia documentation. \" 2.5.2 Introducing DataFrames.jl When dealing with any type of data in large quantities, it is essential to have a framework to organize and manipulate it in an efficient way. If you have previously used Python, you probably came across the Pandas package and dataframes. In Julia, the DataFrames.jl package follows the same idea. Dataframes are objects with the purpose of structuring tabular data in a smart way. You can think of them as a table, a matrix or a spreadsheet. In the dataframe convention, each row is an observation of a vector-type variable, and each column is the complete set of values of a given variable, across all observations. In other words, for a single row, each column represents a a realization of a variable. Let’s see how to construct and load data into a dataframe. There are many ways you can accomplish this. Consider we had some data in a matrix and we want to organize it in a dataframe. First, we are going to create some ‘fake data’ and loading that in a Julia DataFrame, begin using DataFrames, Random Random.seed!(123) fake_data = rand(5, 5) # this creates a 5x5 matrix with random values between 0 # and 1 in each matrix element. df = DataFrame(fake_data) end ## 5×5 DataFrame ## Row │ x1 x2 x3 x4 x5 ## │ Float64 Float64 Float64 Float64 Float64 ## ─────┼─────────────────────────────────────────────────── ## 1 │ 0.768448 0.662555 0.163666 0.463847 0.70586 ## 2 │ 0.940515 0.586022 0.473017 0.275819 0.291978 ## 3 │ 0.673959 0.0521332 0.865412 0.446568 0.281066 ## 4 │ 0.395453 0.26864 0.617492 0.582318 0.792931 ## 5 │ 0.313244 0.108871 0.285698 0.255981 0.20923 As you can see, the column names were initialized with values \\(x1, x2, ...\\). We probably would want to rename them with more meaningful names. To do this, we have the rename!() function. Remember that this function has a bang, so it changes the dataframe in-place, be careful! Below we rename the columns of our dataframe, rename!(df, [&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;]) ## 5×5 DataFrame ## Row │ one two three four five ## │ Float64 Float64 Float64 Float64 Float64 ## ─────┼─────────────────────────────────────────────────── ## 1 │ 0.768448 0.662555 0.163666 0.463847 0.70586 ## 2 │ 0.940515 0.586022 0.473017 0.275819 0.291978 ## 3 │ 0.673959 0.0521332 0.865412 0.446568 0.281066 ## 4 │ 0.395453 0.26864 0.617492 0.582318 0.792931 ## 5 │ 0.313244 0.108871 0.285698 0.255981 0.20923 The first argument of the function is the dataframe we want to modify, and the second an array of strings, each one corresponding to the name of each column. Another way to create a dataframe is by passing a list of variables that store arrays or any collection of data. For example, \" DataFrame(column1=1:10, column2=2:2:20, column3=3:3:30) ## 10×3 DataFrame ## Row │ column1 column2 column3 ## │ Int64 Int64 Int64 ## ─────┼─────────────────────────── ## 1 │ 1 2 3 ## 2 │ 2 4 6 ## 3 │ 3 6 9 ## 4 │ 4 8 12 ## 5 │ 5 10 15 ## 6 │ 6 12 18 ## 7 │ 7 14 21 ## 8 │ 8 16 24 ## 9 │ 9 18 27 ## 10 │ 10 20 30 As you can see, the name of each array is automatically assigned to the columns of the dataframe. Furthermore, you can initialize an empty dataframe and start adding data later if you want, begin df_ = DataFrame(Names = String[], Countries = String[], Ages = Int64[]) df_ = vcat(df_, DataFrame(Names=&quot;Juan&quot;, Countries=&quot;Argentina&quot;, Ages=28)) end ## 1×3 DataFrame ## Row │ Names Countries Ages ## │ String String Int64 ## ─────┼────────────────────────── ## 1 │ Juan Argentina 28 We have used the vcat()function seen earlier to append new data to the dataframe. You can also add a new column very easily, begin df_.height = [1.72] df_ end ## 1×4 DataFrame ## Row │ Names Countries Ages height ## │ String String Int64 Float64 ## ─────┼─────────────────────────────────── ## 1 │ Juan Argentina 28 1.72 You can access data in a dataframe in various ways. One way is by the column name. For example, df.three ## 5-element Array{Float64,1}: ## 0.16366581948600145 ## 0.4730168160953825 ## 0.8654121434083455 ## 0.617491887982287 ## 0.2856979003853177 df.&quot;three&quot; ## 5-element Array{Float64,1}: ## 0.16366581948600145 ## 0.4730168160953825 ## 0.8654121434083455 ## 0.617491887982287 ## 0.2856979003853177 But you can also access dataframe data as if it were a matrix. You can treat columns either as their column number or by their name, df[1,:] ## DataFrameRow ## Row │ one two three four five ## │ Float64 Float64 Float64 Float64 Float64 ## ─────┼───────────────────────────────────────────────── ## 1 │ 0.768448 0.662555 0.163666 0.463847 0.70586 df[1:2, &quot;one&quot;] ## 2-element Array{Float64,1}: ## 0.7684476751965699 ## 0.940515000715187 df[3:5, [&quot;two&quot;, &quot;four&quot;, &quot;five&quot;]] ## 3×3 DataFrame ## Row │ two four five ## │ Float64 Float64 Float64 ## ─────┼─────────────────────────────── ## 1 │ 0.0521332 0.446568 0.281066 ## 2 │ 0.26864 0.582318 0.792931 ## 3 │ 0.108871 0.255981 0.20923 The column names can be accessed by the names() function, names(df) ## 5-element Array{String,1}: ## &quot;one&quot; ## &quot;two&quot; ## &quot;three&quot; ## &quot;four&quot; ## &quot;five&quot; Another useful tool for having a quick overview of the dataframe, typically when in an exploratory process, is the describe() function. It outputs some information about each column, as you can see below, describe(df) ## 5×7 DataFrame ## Row │ variable mean min median max nmissing eltype ## │ Symbol Float64 Float64 Float64 Float64 Int64 DataType ## ─────┼─────────────────────────────────────────────────────────────────────── ## 1 │ one 0.618324 0.313244 0.673959 0.940515 0 Float64 ## 2 │ two 0.335644 0.0521332 0.26864 0.662555 0 Float64 ## 3 │ three 0.481057 0.163666 0.473017 0.865412 0 Float64 ## 4 │ four 0.404907 0.255981 0.446568 0.582318 0 Float64 ## 5 │ five 0.456213 0.20923 0.291978 0.792931 0 Float64 To select data following certain conditions, you can use the filter() function. Given some condition, this function will throw away all the rows that don’t evaluate the condition to true. This condition is expressed as an anonymous function and it is written in the first argument. In the second argument of the function, the dataframe where to apply the filtering is indicated. In the example below, all the rows that have their ‘one’ column value greater than \\(0.5\\) are filtered. filter(col -&gt; col[1] &lt; 0.5, df) ## 2×5 DataFrame ## Row │ one two three four five ## │ Float64 Float64 Float64 Float64 Float64 ## ─────┼────────────────────────────────────────────────── ## 1 │ 0.395453 0.26864 0.617492 0.582318 0.792931 ## 2 │ 0.313244 0.108871 0.285698 0.255981 0.20923 A very usual application of dataframes is when dealing with CSV data. In case you are new to the term, CSV stands for Comma Separated Values. As the name indicates, these are files where each line is a data record, composed by values separated by commas. In essence, a way to store tabular data. A lot of the datasets around the internet are available in this format, and naturally, the DataFrame.jl package is well integrated with it. As an example, consider the popular Iris flower dataset. This dataset consists of samples of three different species of plants. The samples correspond to four measured features of the flowers: length and width of the sepals and petals. To work with CSV files, the package CSV.jl is your best choice in Julia. Loading a CSV file is very easy once you have it downloaded. Consider the following code, begin using CSV iris_df = CSV.File(&quot;./02_julia_intro/data/Iris.csv&quot;) |&gt; DataFrame end ## 150×6 DataFrame ## Row │ Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Specie ⋯ ## │ Int64 Float64 Float64 Float64 Float64 String ⋯ ## ─────┼────────────────────────────────────────────────────────────────────────── ## 1 │ 1 5.1 3.5 1.4 0.2 Iris-s ⋯ ## 2 │ 2 4.9 3.0 1.4 0.2 Iris-s ## 3 │ 3 4.7 3.2 1.3 0.2 Iris-s ## 4 │ 4 4.6 3.1 1.5 0.2 Iris-s ## 5 │ 5 5.0 3.6 1.4 0.2 Iris-s ⋯ ## 6 │ 6 5.4 3.9 1.7 0.4 Iris-s ## 7 │ 7 4.6 3.4 1.4 0.3 Iris-s ## 8 │ 8 5.0 3.4 1.5 0.2 Iris-s ## ⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋱ ## 144 │ 144 6.8 3.2 5.9 2.3 Iris-v ⋯ ## 145 │ 145 6.7 3.3 5.7 2.5 Iris-v ## 146 │ 146 6.7 3.0 5.2 2.3 Iris-v ## 147 │ 147 6.3 2.5 5.0 1.9 Iris-v ## 148 │ 148 6.5 3.0 5.2 2.0 Iris-v ⋯ ## 149 │ 149 6.2 3.4 5.4 2.3 Iris-v ## 150 │ 150 5.9 3.0 5.1 1.8 Iris-v ## 1 column and 135 rows omitted Here we used the pipeline operator |&gt;, which is mainly some Julia syntactic sugar. It resembles the flow of information. First, the CSV.File()function, loads the CSV file and creates a CSV File object, that is passed to the DataFrame()function, to give us finally a dataframe. Once you have worked on the dataframe cleaning data or modifying it, you can write a CSV text file from it and in this way, you can share your work with other people. For example, consider I want to filter one of the species of plants, ‘Iris-setosa,’ and then I want to write a file with this modified data to share it with someone, begin filter!(row -&gt; row.Species != &quot;Iris-setosa&quot;, iris_df) CSV.write(&quot;./02_julia_intro/data/modified_iris.csv&quot;, iris_df) end; Plotting Dataframes data is very easy. Suppose we want to plot the flower features from the iris dataset, all in one single plot. These features correspond to the columns two to five of the dataframe. Thinking about it as a matrix, you can access these data by selecting all the rows for each of the corresponding columns. In the code below, a loop is performed over the columns of interest. The plot() statement, with no arguments, is a way to create an empty instance of a plot, like a blank canvas. This empty plot will be successively overwritten by each call to plot!(). Finally, we make a call to current(), to display the plot. You may be wondering why this is necessary. Notice that all the plotting happens inside a loop, hence the plotting iterations are not displayed. It is more efficient to display the finished plot when the loop is over than to update each plot as it overwrites the previous one. begin gr() plot() for i in 2:5 plot!(iris_df[:,i], legend=false) end xlabel!(&quot;Flower&quot;) ylabel!(&quot;Centimeters (cm)&quot;) title!(&quot;Flower features&quot;) current() end 2.6 Summary In this chapter we have introduced the Julia language, the motivations behind its creation, features, installation and basic building blocks for writing some code. First we discussed some basic Julia operators and datatypes. Some special features of the language such as how to write different kinds of arrays and broadcasting were detailed. We then followed with an overview of how functions work in Julia, and how to make your own. Finally, we introduced some packages of the Julia ecosystem, mainly the Plots.jl package for plotting and changing backends, and DataFrames.jl for data organization and manipulation. 2.7 References Julia’s website Julia REPL Learn X in Y minutes Introducing Julia Julia Plots - Backends Data Science with Julia Comma Separated Values Iris Dataset 2.8 Give us feedback This book is currently in a beta version. We are looking forward to getting feedback and criticism: Submit a GitHub issue here. Mail us to martina.cantaro@lambdaclass.com Thank you! "],["probability-introduction.html", "Chapter 3 Probability introduction", " Chapter 3 Probability introduction "],["spam-filter.html", "Chapter 4 Spam filter 4.1 Naive Bayes: Spam or Ham? 4.2 Summary 4.3 References 4.4 Give us feedback", " Chapter 4 Spam filter 4.1 Naive Bayes: Spam or Ham? begin using CSV using DataFrames using Distributions using TextAnalysis using Languages using MLDataUtils using Plots using Images end We all hate spam emails. How can Bayes help us with this? What we will be introducing in this chapter is a simple yet effective way of using Bayesian probability to make a spam filter of emails based on their content. There are many possible origins of the ‘Spam’ word. Some people suggest Spam is a satirized way to refer to ‘fake meat.’ Hence, in the context of emails, this would just mean ‘fake emails.’ It makes sense, but the real story is another one. The origin of this term can be tracked to the 1970s, where the British surreal comedy troupe Monty Python gave life to it in a sketch of their Monty Python’s Flying Circus series. In the sketch, a customer wants to make an order in a restaurant, but all the restaurant’s items have spam in them. As the waitress describes the food, she repeats the word spam, and as this happens, a group of Vikings sitting on another table nearby start singing ’Spam, spam, spam, spam, spam, spam, spam, spam, lovely spam! Wonderful spam! until they are told to shut up. Although the exact moment where this was first translated to different types of internet messages such as emails or chat messages can’t be stated clearly, it is a well known fact that users in each of these messaging instances chose the word ‘spam’ as a reference to Monty Python’s sketch, where spam was itself something unwanted, popping all over the menu and annoyingly trying to drown out the conversation. Now that we have made some historical overview of the topic, we can start designing our spam filter. One of the most important things for the filter to work properly will be to feed it with some good training data. What do we mean by this? In this context, we mean to have a large enough corpus of emails classified as spam or ham (that’s the way no-spam emails are called!), that the emails are collected from an heterogeneous group of persons (spam and ham emails will be not be the same from a software developer, a social scientist or a graphics designer), and that the proportion of spam vs. ham in our data is somewhat representative of the real proportion of mails we receive. Fortunately, there are a lot of very good datasets available online. We will be using one from Kaggle, a community of data science enthusiasts and practitioners who publish datasets, make competitions and share their knowledge. This dataset is already a bit pre-processed, as you will probably notice. It consists of 5172 emails, represented by the rows of a matrix or DataFrame. Each column represents a word from the 3000 most frequent words in all mails, and picking a row and a column will tell us how many times a given word appears in a particular email. The last column indicates a 0 for ham emails and 1 for spam. Let’s give it a look: raw_df = CSV.File(&quot;./04_naive_bayes/data/emails.csv&quot;) |&gt; DataFrame ## 5172×3002 DataFrame ## Row │ Email No. the to ect and for of a you ho ⋯ ## │ String Int64 Int64 Int64 Int64 Int64 Int64 Int64 Int64 In ⋯ ## ──────┼───────────────────────────────────────────────────────────────────────── ## 1 │ Email 1 0 0 1 0 0 0 2 0 0 ⋯ ## 2 │ Email 2 8 13 24 6 6 2 102 1 27 ## 3 │ Email 3 0 0 1 0 0 0 8 0 0 ## 4 │ Email 4 0 5 22 0 5 1 51 2 10 ## 5 │ Email 5 7 6 17 1 5 2 57 0 9 ⋯ ## 6 │ Email 6 4 5 1 4 2 3 45 1 0 ## 7 │ Email 7 5 3 1 3 2 1 37 0 0 ## 8 │ Email 8 0 2 2 3 1 2 21 6 0 ## ⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋱ ## 5166 │ Email 5166 1 0 1 0 3 1 12 1 0 ⋯ ## 5167 │ Email 5167 1 0 1 1 0 0 4 0 0 ## 5168 │ Email 5168 2 2 2 3 0 0 32 0 0 ## 5169 │ Email 5169 35 27 11 2 6 5 151 4 3 ## 5170 │ Email 5170 0 0 1 1 0 0 11 0 0 ⋯ ## 5171 │ Email 5171 2 7 1 0 2 1 28 2 0 ## 5172 │ Email 5172 22 24 5 1 6 5 148 8 2 ## 2993 columns and 5157 rows omitted What we are facing here is a classification problem, and we will code from scratch and use a supervised learning algorithm to find a solution with the help of Bayes’ theorem. In particular, we will be using naive Bayes. What we are going to do is to treat each email just as a collection of words. The particular relationship between words and the context will not be taken into account here. Our strategy will be to estimate a probability of an incoming email of being ham or spam and making a decision based on that. Our general approach can be summarized as: \\(P(spam|email) \\propto P(email|spam)P(spam)\\) \\(P(ham|email) \\propto P(email|ham)P(ham)\\) Where we use \\(\\propto\\) sign instead of \\(=\\) sign because the denominator from Bayes’ theorem is missing, but we won’t need to calculate it as it is the same for both probabilities and all we are going to care about is a comparison of these two probabilities. So what do \\(P(email|spam)\\) and \\(P(email|ham)\\) mean and how do we calculate them? To answer this question, we have to remember that we are interpreting each email just as a collection of words, with no importance on their order within the text. In this naive approach, the semantics are not taken into account. In this scope, the conditional probability \\(P(email|spam)\\) just means the probability that a given email can be generated with the collection of words that appear in the spam category of our data. If this still sounds a bit confusing, let’s make a quick example. Consider for a moment that our training spam set of emails consists just of these three emails: email 1: ‘are you interested in buying my product?’ email 2: ‘congratulations! you’ve won $1000!’ email 3: ‘check out this product!’ Also consider we have a new email and we want to ask ourselves what \\(P(email|spam)\\). This new email looks like this: new email: ‘apply and win all this products!’ As we already said, \\(P(email|spam)\\) stands for the plausibility of the new email being generated by the words we encountered in our training spam email set. We can see that words like ‘win’ –which in our training set appears in the form of ‘won,’ but there is a standard technique in linguistics named Lemmatization, which groups together inflected forms of a word, letting us consider ‘win’ and ‘won’ as the same word– and ‘product’ appear rather commonly in our training data. So we will expect \\(P(email|spam)\\) to be relatively high in this fake and simple example, as it contains words that are repeated among our spam emails data. Let’s make all this discussion a bit more explicitly mathematical. The simplest way to write this in a mathematical way is to take each word appearing in the email and calculate the probability of it appearing in spam emails and ham emails. Then, we do this for each word in the email and finally multiply them, \\(P(email|spam) = \\prod_{i=1}^{n}P(word_i|spam)\\) \\(P(email|ham) = \\prod_{i=1}^{n}P(word_i|ham)\\) The multiplication of each of the word probabilities here stands from the supposition that all the words in the email are statistically independent. We have to stress that this is not necessarily true, and most likely false. Words in a language are never independent from one another, but this simple assumption seems to be enough for the level of complexity our problem requires. Let’s start building a solution for our problem and the details will be discussed later. # preprocessing of the data begin all_words = names(raw_df)[2:end-1]; all_words_text = StringDocument(join(all_words, &quot; &quot;)) prepare!(all_words_text, strip_articles) prepare!(all_words_text, strip_pronouns) vocabulary = filter(x -&gt; x != &quot;&quot;, split(TextAnalysis.text(all_words_text))) clean_words_df = raw_df[!, vocabulary] # the columns of this matrix will be each mail now data_matrix = Matrix(clean_words_df)&#39; end; First, we would like to filter some words that are very common in the english language, such as articles and pronouns, and that will most likely add noise rather than information to our classification algorithm. For this we will use two Julia packages that are specially designed for working with texts of any type. These are Languages.jl and TextAnalysis.jl. A good practice when dealing with models that learn from data like the one we are going to implement, is to divide our data in two: a training set and a testing set. We need to measure how good our model is performing, so we will train it with some data, and test it with some other data the model has never seen. This way we may be sure that the model is not tricking us. In Julia, the package MLDataUtils has some nice functionalities for data manipulations like this. We will use the functions splitobs to split our dataset in a train set and a test set and shuffleobs to randomize the order of our data in the split. It is important also to pass a labels array to our split function so that it knows how to properly split our dataset. begin # splitting of the data in a train and test set labels = raw_df.Prediction (x_train, y_train), (x_test, y_test) = splitobs(shuffleobs((data_matrix, labels)), at = 0.7) end; Now that we have our data clean and splitted for training and testing, let’s return to the details of the calculations. The probability of a particular word, given that we have a spam email, can be calculated like so, \\(P(word_i|spam) = \\frac{N_{word_i|spam} + \\alpha}{N_{spam} + \\alpha N_{vocabulary}}\\) \\(P(word_i|ham) = \\frac{N_{word_i|ham} + \\alpha}{N_{ham} + \\alpha N_{vocabulary}}\\) With these formulas in mind, we now know exactly what we have to calculate from our data. We are going to need the numbers \\(N_{word_i|spam}\\) and \\(N_{word_i|ham}\\) for each word, that is, the number of times that a given word \\(w_i\\) is used in the spam and ham categories, respectively. Then \\(N_{spam}\\) and \\(N_{ham}\\) are the total number of times that words are used in the spam and ham categories (considering all the repetitions of the same words too), and finally, \\(N_{vocabulary}\\) is the total number of unique words in the dataset. \\(α\\) is just a smoothing parameter, so that probability of words that, for example, are not in the spam category don’t give 0 probability. As all this information will be particular for our dataset, so a clever way to aggregate all this is to use a Julia struct, and we can define the attributes of the struct that we will be using over and over for the prediction. Below we can see the implementation. The relevant attributes of the struct will be words_count_ham and words_count_spam, two dictionaries containing the frequency of appearance of each word in the ham and spam datasets, N_ham and N_spam the total number of words appearing in each category, and finally vocabulary, an array with all the unique words in our dataset. The line BayesSpamFilter() = new() is just the constructor of this struct. When we instantiate the filter, all the attributes will be undefined and we will have to define some functions to fill these variables with values relevant to our particular problem. mutable struct BayesSpamFilter words_count_ham::Dict{String, Int64} words_count_spam::Dict{String, Int64} N_ham::Int64 N_spam::Int64 vocabulary::Array{String} BayesSpamFilter() = new() end Now we are going to proceed to define some functions that will be important for our filter implementation. The function word_data below will help for counting the occurrences of each word in ham and spam categories. function words_count(word_data, vocabulary, labels, spam=0) # word_data is a matrix where each column is an email and each row is a word count_dict = Dict{String, Int64}() n_emails = size(word_data)[2] for (i, word) in enumerate(vocabulary) count_dict[word] = sum([word_data[i,j] for j in 1:n_emails if labels[j]==spam]) end return count_dict end ## words_count (generic function with 2 methods) Next, we will define the fit! function for our spam filter struct. We are using the bang(!) convention for the functions that modify in-place their arguments, in this case, the spam filter struc itself. This will be the function that will fit our model to the data, a typical procedure in Data Science and Machine Learning areas. This fit function will use mainly the words_count function defined before to fill all the undefined parameters in the filter’s struct. function fit!(model::BayesSpamFilter, x_train, y_train, voc) model.vocabulary = voc model.words_count_ham = words_count(x_train, model.vocabulary, y_train, 0) model.words_count_spam = words_count(x_train, model.vocabulary, y_train, 1) model.N_ham = sum(values(model.words_count_ham)) model.N_spam = sum(values(model.words_count_spam)) return end ## fit! (generic function with 1 method) Now it is time to instantiate our spam filter and fit the model to the data. We do this with our training data so then we can measure how well it is working in our test data. begin # here we instantiate the Bayes filter and then we fit it to our data spam_filter = BayesSpamFilter() fit!(spam_filter, x_train, y_train, vocabulary) end We are now almost ready to make some predictions and test our model. The function below is just the implementation of the formula TAL that we have already talked about. It will be used internally by the next function defined, spam_predict, which will receive a new email –the one we would want to classify as spam or ham–, our fitted model, and two parameters, α which we have already discussed in the formula for \\(P(word_i|spam)\\) and \\(P(word_i|ham)\\), and tol. We saw that the calculation for \\(P(email|spam)\\) and \\(P(email|ham)\\) required the multiplication of each \\(P(word_i|spam)\\) and \\(P(word_i|ham)\\) term. When mails are too large, i.e., they have a lot of words, this multiplication may lead to very small probabilities, up to the point that the computer interprets those probabilities as zero. This can’t happen, as we need values of \\(P(email|spam)\\) and \\(P(email|ham)\\) that are larger than zero so we can multiply them by \\(P(spam)\\) and \\(P(ham)\\) respectively and compare these values to make a prediction. The parameter tol is the maximum tolerance for the number of unique words in an email. If this number is greater than the parameter tol, only the most frequent words will be considered and the rest will be neglected. How many of these most frequent words? the first ‘tol’ most frequent words! function word_spam_probability(word, words_count_ham, words_count_spam, N_ham, N_spam, n_vocabulary, α) ham_prob = (words_count_ham[word] + α)/(N_ham + α*(n_vocabulary)) spam_prob = (words_count_spam[word] + α)/(N_spam + α*(n_vocabulary)) return ham_prob, spam_prob end ## word_spam_probability (generic function with 1 method) function spam_predict(email, model::BayesSpamFilter, α, tol=100) ngrams_email = ngrams(StringDocument(email)) email_words = keys(ngrams_email) n_vocabulary = length(model.vocabulary) ham_prior = model.N_ham/(model.N_ham + model.N_spam) spam_prior = model.N_spam/(model.N_ham + model.N_spam) if length(email_words) &gt; tol word_freq = values(ngrams_email) sort_idx = sortperm(collect(word_freq), rev=true) email_words = collect(email_words)[sort_idx][1:tol] end email_ham_probability = BigFloat(1) email_spam_probability = BigFloat(1) for word in intersect(email_words, model.vocabulary) word_ham_prob, word_spam_prob = word_spam_probability(word, model.words_count_ham, model.words_count_spam, model.N_ham, model.N_spam, n_vocabulary, α) email_ham_probability *= word_ham_prob email_spam_probability *= word_spam_prob end return ham_prior*email_ham_probability, spam_prior*email_spam_probability end ## spam_predict (generic function with 2 methods) begin # this cell is just for testing mail_num = 56 email = string([repeat(string(word, &quot; &quot;), n) for (word, n) in zip(vocabulary, x_test[:,mail_num])]...); #nospam_prob, spam_prob = spam_predict(email, train_nospam, train_spam, N_nospam, N_spam, vocabulary, 1) ham_prob, spam_prob = spam_predict(email, spam_filter, 1) @show ham_prob @show spam_prob y_test[mail_num] end ## ham_prob = 1.923496233808638269440911387356602371032004828612496882426453835893773412005201e-292 ## spam_prob = 1.877426487200063072344769025907711319192235063954185153248420158172208626590648e-285 ## 1 Finally we arrived to the point of actually testing our model. This is what the function below is all about. We feed it with our model fitted with the training data, and the test data we had splitted at the beginning, as well as with the labels of the classification of this data. This function makes a prediction for each email in our test data, using the values of our model and then checks if the prediction was right. We count all the correct predictions and then we divide this number by the total amount of mails, giving us an accuracy measurement. #This function classify each mail into Ham(0) or Spam(1) function get_predictions(x_test, y_test, model::BayesSpamFilter, α, tol=200) N = length(y_test) predictions = Array{Int64, 1}(undef,N) for i in 1:N email = string([repeat(string(word, &quot; &quot;),N) for (word,N) in zip(model.vocabulary, x_test[:, i])]...) pham, pspam = spam_predict(email, model, α, tol) pred = argmax([pham, pspam]) - 1 predictions[i] = pred end return predictions end ## get_predictions (generic function with 2 methods) predictions = get_predictions(x_test, y_test, spam_filter, 1) Here we can see how our model classifies the first 5 mails, 0 for ham and 1 for spam. predictions[1:5] ## 5-element Array{Int64,1}: ## 0 ## 1 ## 0 ## 0 ## 1 function spam_filter_accuracy(predictions, actual) N = length(predictions) correct = sum(predictions .== actual) accuracy = correct /N return accuracy end ## spam_filter_accuracy (generic function with 1 method) As you can see below, the model (at least under this simple metric) is performing very well! An accuracy of about 0.95 is quite astonishing for a model so naive and simple, but it works! spam_filter_accuracy(predictions, y_test) ## 0.9452319587628866 But we have to take into account one more thing. Our model classifies mails into spam or ham and the amount of ham mails is considerably higher than the spam ones. Let’s see the percentages sum(x_train)/length(x_train) ## 0.3559861646315985 So we know that only the 0.36% of the mails in the train section are spam. This classification problems where there is an unequal distribution of classes in the dataset are called Imbalanced. So a good way to see how our model is performing is to construct a confusion matrix. A Confusion matrix is an N x N matrix, where N is the number of target classes. The matrix compares the actual target values with those predicted by the our model. Lets construct one for our model: function splam_filter_confusion_matrix(y_test,predictions) #We create the matrix and calculated their values confusion_matrix= [0 0 ; 0 0] confusion_matrix[1,1] = sum(isequal(y_test[i],0) &amp; isequal(predictions[i],0) for i in 1:length(y_test)) confusion_matrix[1,2] = sum(isequal(y_test[i],1) &amp; isequal(predictions[i],0) for i in 1:length(y_test)) confusion_matrix[2,1] = sum(isequal(y_test[i],0) &amp; isequal(predictions[i],1) for i in 1:length(y_test)) confusion_matrix[2,2] = sum(isequal(y_test[i],1) &amp; isequal(predictions[i],1) for i in 1:length(y_test)) #Now we convert the confusion matrix into a DataFrame confusion_df = DataFrame(Prediction = String[], Ham_mail = Int64[], Spam_mail = Int64[]) confusion_df = vcat(confusion_df,DataFrame(Prediction = &quot;Model predicted Ham&quot;, Ham_mail = confusion_matrix[1,1] , Spam_mail = confusion_matrix[1,2])) confusion_df = vcat(confusion_df ,DataFrame(Prediction = &quot;Model predicted Spam&quot;, Ham_mail = confusion_matrix[2,1] , Spam_mail = confusion_matrix[2,2])) return confusion_df end ## splam_filter_confusion_matrix (generic function with 1 method) confusion_matrix = splam_filter_confusion_matrix(y_test[:],predictions) ## 2×3 DataFrame ## Row │ Prediction Ham_mail Spam_mail ## │ String Int64 Int64 ## ─────┼─────────────────────────────────────────── ## 1 │ Model predicted Ham 1065 35 ## 2 │ Model predicted Spam 50 402 So now we can calculate the accuracy of the model segmented by category. ham_accuracy = confusion_matrix[1,&quot;Ham_mail&quot;]/(confusion_matrix[1,&quot;Ham_mail&quot;] + confusion_matrix[2,&quot;Ham_mail&quot;]) ## 0.9551569506726457 spam_accuracy = confusion_matrix[2,&quot;Spam_mail&quot;]/(confusion_matrix[1,&quot;Spam_mail&quot;] + confusion_matrix[2,&quot;Spam_mail&quot;]) ## 0.919908466819222 4.2 Summary In this chapter, we have used a naive-bayes approach to build a simple email spam filter. First, the dataset and the theoretical framework were introduced. Using Bayes’ theorem and the data available, we assigned probability of belonging to a spam or ham email to each word of the email dataset. The probability of a new email being classified as spam is therefore the product of the probabilities of each of its constituent words. Later, the data was pre-processed and a struct was defined for the spam filter object. Functions were then implemented to fit the spam filter object to the data. Finally, we evaluated our model performance calculating the accuracy and making a confusion matrix. 4.3 References What is Spam Filtering? Artificial Intelligence in Python: A Comprehensive Guide to Building Intelligent Apps for Python Beginners and Developers Data Algorithms: Recipes for scaling up with Hadoop and Spark Doing Data Science: Straight talk from the frontline How the word ‘Spam’ came to mean ‘Junk Message’ Monty Python Sketch - YouTube 4.4 Give us feedback This book is currently in a beta version. We are looking forward to getting feedback and criticism: Submit a GitHub issue here. Mail us to martina.cantaro@lambdaclass.com Thank you! "],["probabilistic-programming.html", "Chapter 5 Probabilistic programming", " Chapter 5 Probabilistic programming "],["escaping-from-mars.html", "Chapter 6 Escaping from Mars 6.1 Calculating the constant g of Mars 6.2 Optimizing the throwing angle 6.3 Summary 6.4 Give us feedback", " Chapter 6 Escaping from Mars Suppose you landed on Mars by mistake and you want to leave that rocky planet and return home. To escape from a planet you need a very important piece of information: the escaping velocity from the planet. What on Mars is the escape velocity? We are going to use the same experiment that Newton thought when thinking about escaping from the gravity of Earth. Gravity pulls us down, so if we shoot a cannonball, as in the sketch shown below, what will happen? For some velocities, the cannonball will return to Earth, but there’s a velocity at which it scapes since the gravitational pull is not enough to bring it back to the surface. That velocity is called escape velocity. begin using Images using Plots im_0 = load(&quot;./06_gravity/images/cannonball_.png&quot;) plot(im_0) #img_small = imresize(im_0, ratio=1/3) end To simplify, we approximate the escape velocity as: \\(v_{escape}=\\sqrt{2*g_{planet}*r_{planet}}\\) where \\(r\\) is the radius of the planet and \\(g\\) the constant of gravity at the surface. Suppose that we remember from school that the velocity of scape from Earth is \\(11\\frac{km}{s}\\) and that the radius of Mars if half of the earth’s. We remember that the gravity of Earth at its surface is \\(9.8\\frac{m}{s^2}\\), so all we need to estimate the escape velocity of Mars is the gravity of the planet at its surface. So we decide to make an experiment and gather some data. But what exactly do you need to measure? Let’s see. We are going to calculate the constant \\(g_{mars}\\) just throwing stones. We are going to explain a bit the equations regarding the experiment. The topic we need to revisit is Proyectile Motion. ## Proyectile Motion Gravity pulls us down to Earth, or in our case, to Mars. This means that we have an acceletation, since there is a force. Recalling the newton equation: \\(\\overrightarrow{F} = m * \\overrightarrow{a}\\) where \\(m\\) is the mass of the object, \\(\\overrightarrow{F}\\) is the force(it’s what make us fall) and \\(\\overrightarrow{a}\\) is the acceleration, in our case is what we call gravity \\(\\overrightarrow{g}\\). The arrow \\(\\overrightarrow{}\\) over the letter means that the quantity have a direction in space, in our case, gravity is pointing to the center of the Earth, or Mars. How can we derive the motion of the stones with that equation? In the figure below we show a sketch of the problem: We have the 2 axis, \\(x\\) and \\(y\\), the \\(x\\) normally is parallel to the ground and the \\(y\\) axis is perpendicular, pointing to the sky. We also draw the initial velocity \\(v_0\\) of the proyectile, and the angle \\(\\theta\\) with respect to the ground. Also it’s important to notice that the gravity points in the opposite direction of the \\(y\\) axis. begin im_1 = load(&quot;./06_gravity/images/trajectory.png&quot;) plot(im_1) end But what are the trayectory of the proyectile? and how the coordinates \\(x\\) and \\(y\\) evolve with time? If we remember from school, the equations of \\(x\\) and \\(y\\) over time is: \\(x(t) = v_0*t*cos(θ)\\) \\(y(t) = v_0*t*sin(θ) -\\frac{g*t^2}{2}\\) where \\(t\\) is the time at which we want to know the coordinates. What the equations tell us? If we see the evolution of the projectile in the \\(x\\) axis only, it follows a straight line (until it hits the ground) and in the \\(y\\) axis the movement follows a parabola, but how we interpret that? We can imagine what happens if we trow a stone to the sky: the stone starts to go up and then, at some point, it reaches the highest position it can go. Then, the stone starts to go down. How does the velocity evolve in this trajectory? Since the begining, the velocity starts decreasing until it has the value of 0 at the highest point, where the stone stops for a moment, then it changes its direction and start to increase again, pointing towards the ground. Ploting the evolution of the height of the stone, we obtain the plot shown below. We see that, at the begining the stone starts to go up fast and then it slows down. We see that for each value of \\(y\\) there are 2 values of \\(t\\) that satisfies the equation, thats because the stone pass twice for each point, except for the highest value of \\(y\\). begin im_3 = load(&quot;./06_gravity/images/img90_.png&quot;) plot(im_3) end So, in the example we ahve just explained, we have that the throwing angle is θ=90°, so sin(90°)=1, the trajectory in \\(y\\) becomes: \\(y(t) = v_0*t -\\frac{g*t^2}{2}\\) And the velocity, which is the derivative of the above equation becomes: \\(v_{y}(t) = v_{0} -g*t\\) Those two equations are the ones plotted in the previous sketch, a parabola and a straight line that decreases with time. It’s worth to notice that at each value \\(y\\) of the trajectory, the velocity could have 2 values, just differing in its sign, meaning it can has 2 directions, but with the same magnitude. So keep in mind that when you throw an object to the sky, when it returns to you, the velocity will be the same with the one you threw it. 6.1 Calculating the constant g of Mars Now that we have understood the equations we will work with, we ask: how do we set the experiment and what do we need to measure? The experiment set up will go like this: - One person will be throwing stones with an angle. - The other person will be far, watching from some distance, measuring the time since the other throw the stone and it hits the ground. The other measurement we will need is the distance Δx the stone travelled. - Also, for the first iteration of the experiment, suppose we only keep the measurements with and initial angle θ~45° (we will loosen this constrain in a bit). begin im = load(&quot;./06_gravity/images/sketch_2.png&quot;) plot(im) end Suppose we did the experiment and we have measured then the 5 points, Δx and Δt, shown below: Δx_measured = [25.94, 38.84, 52.81, 45.54, 17.24] ## 5-element Array{Float64,1}: ## 25.94 ## 38.84 ## 52.81 ## 45.54 ## 17.24 t_measured = [3.91, 4.57, 5.43, 4.85, 3.15] ## 5-element Array{Float64,1}: ## 3.91 ## 4.57 ## 5.43 ## 4.85 ## 3.15 Now, how we estimate the constant g from those points? Using the equations of the trajectory, when the stone hits the ground, y(t) = 0, since we take the start of the \\(y\\) coordinate in the ground (negleting the initial height with respect to the maximum height), so finding the other then the initial point that fulfill this equation, we find that: \\(t_{f} = \\frac{2*v_{0}*sin(θ)}{g}\\) where \\(t_{f}\\) is the time at which the stone hits the ground, the time we have measured. And replacing this time in the x(t) equation we find that: \\(Δx=t_{f}*v_{0}*cos(θ)\\) where Δx is the distance traveled by the stone. So, solving for \\(v_{0}\\)m, the initial velocity, an unknown quantity, we have: \\(v_{0}=\\frac{Δx}{t_{f}cos(θ)}\\) Then replacing it in the equation of \\(t_{f}\\) and solving for \\(\\Delta x\\) we obtain: \\(Δx=\\frac{g*t_{f}^2}{2*tg(θ)}\\) So, the model we are going to propose is a linear regression. A linear equation has the form: \\(y=m*x +b\\) where \\(m\\) is the slope of the curve and \\(b\\) is called the intercept. In our case, if we take \\(x\\) to be \\(\\frac{t_{f}^2}{2}\\), the slope of the curve is g and the intercep is 0. So, in our linear model we are going to propose that each point in the curve is: \\(\\mu = m*x + b\\) \\(y \\sim Normal(\\mu,\\sigma^2)\\) So, what this says is that each point of the regresion is drawn from a gaussian distribution with its center correnponding with a point in the line, as shown in the plot below. begin im_4 = load(&quot;./06_gravity/images/line_.png&quot;) plot(im_4) end So, our linear model will be: \\(g \\sim Distribution\\_to\\_be\\_proposed()\\) \\(\\mu[i] = g*\\frac{t_{f}^2[i]}{2}\\) \\(\\Delta x[i]= Normal(\\mu[i],\\sigma^2)\\) Where \\(g\\) has a distribution we will propose next. The first distribution we are going to propose is a uniform distribution for g, between the values of 0 and 10 begin using Turing using StatsPlots plot(Uniform(0,10),xlim=(-1,11), ylim=(0,0.2), legend=false, fill=(0, .5,:lightblue)) title!(&quot;Uniform prior distribution for g&quot;) xlabel!(&quot;g_mars&quot;) ylabel!(&quot;Probability&quot;) end Now we define the model in Turing and sample from the posterior distribution. begin @model gravity_uniform(t_final, x_final, θ) = begin # The number of observations. g ~ Uniform(0,10) μ = g .* (t_final.*t_final./2) N = length(t_final) for n in 1:N x_final[n] ~ Normal(μ[n], 10) end end; end begin iterations = 10000 ϵ = 0.05 τ = 10 end; begin θ = 45 chain_uniform = sample(gravity_uniform(t_measured, Δx_measured, θ), HMC(ϵ, τ), iterations, progress=false); end; Plotting the posterior distribution for p we that the values are mostly between 2 and 5, with the maximun near 3,8. Can we narrow the values we obtain? begin histogram(chain_uniform[:g], xlim=[1,6], legend=false, normalized=true) xlabel!(&quot;g_mars&quot;) title!(&quot;Posterior distribution for g with uniform distribution&quot;) end As a second obtion, we can propose a Gaussian distribution instead of a uniforn distribution for \\(g\\), like the one shown below, with a mean of 5 and a variance of 2, and let the model update its beliefs with the points we have. begin plot(Normal(5,2), legend=false, fill=(0, .5,:lightblue)) title!(&quot;Normal prior distribution for g&quot;) xlabel!(&quot;g_mars&quot;) ylabel!(&quot;Probability&quot;) end We define then the model with a gaussian distribution as a prior for \\(g\\): begin @model gravity_normal(t_final, x_final, θ) = begin # The number of observations. N = length(t_final) g ~ Normal(6,2) μ = g .* (t_final.*t_final./2) for n in 1:N x_final[n] ~ Normal(μ[n], 3) end end; end Now we sample values from the posterior distribution and plot and histogram with the values obtained: begin chain_normal = sample(gravity_normal(t_measured, Δx_measured, θ), HMC(ϵ, τ), iterations, progress=false); end; begin histogram(chain_normal[:g], xlim=[3,4.5],legend=false, normalized=true) xlabel!(&quot;g_mars&quot;) title!(&quot;Posterior distribution for g with Normal distribution&quot;) end We see that the plausible values for the gravity have a clear center in 3.7 and now the distribution is narrower, that’s good, but we can do better. If we observe the prior distribution proposed of \\(g\\), we see that some values are negative, which has no sense because if that would the case when you trow the stone, it would go up and up, escaping from the planet. We propose then a new model for not allowing the negative values to happen. The distribution we are interested in is a LogNormal distribution. In the plot below is the prior distribution for g, a LogNormal distribution with mean 1.5 and variance of 0.5. begin plot(LogNormal(1,0.5), xlim=(0,10), legend=false, fill=(0, .5,:lightblue)) title!(&quot;Prior LogNormal Distribution for g&quot;) ylabel!(&quot;Probability&quot;) xlabel!(&quot;g&quot;) end The model gravity_lognormal defined below has now a LogNormal prior. We sample the posterior distribution after updating with the data measured. begin @model gravity_lognormal(t_final, x_final, θ) = begin # The number of observations. N = length(t_final) g ~ LogNormal(0.5,0.5) μ = g .* (t_final.*t_final./2) for n in 1:N x_final[n] ~ Normal(μ[n], 3) end end; end begin chain_lognormal = sample(gravity_lognormal(t_measured, Δx_measured, θ), HMC(ϵ, τ), iterations, progress=false); end; begin histogram(chain_lognormal[:g], xlim=[3,4.5], legend=false, normalized=true) xlabel!(&quot;g_mars&quot;) title!(&quot;Posterior distribution for g with LogNormal distribution&quot;) end 6.2 Optimizing the throwing angle Now that we have a good understanding of the equations and the overall problem, we are going to add some difficulties and we will loosen a constrain we have imposed: Suppose that the device employed to measure the angle has an error of 15°, no matter the angle. We want to know what are the most convenient angle to do the experiment and to measure or if it doesn’t matter. To do the analysis we need to see how the angle influence the computation of \\(g\\), so solving the equation for \\(g\\) we have: \\(g = \\frac{2*tg(\\theta)*\\Delta x}{t^{2}_f}\\) We can plot then the tangent of θ, with and error of 15° and see what is its maximum and minimun value: begin angles = 0:0.1:70 error = 15/2 μ = tan.(deg2rad.(angles)) ribbon = tan.(deg2rad.(angles .+ error)) - μ plot(angles, μ, ribbon=ribbon, color=&quot;lightblue&quot;, legend=false) ylabel!(&quot;tan(θ)&quot;) xlabel!(&quot;θ [deg]&quot;) title!(&quot;tan(θ) and its error&quot;) end But we don’t care about the absolute value of the error, we want the relavite error, so plotting the percentual error we have: begin er= tan.(deg2rad.(angles .+ error)) .- tan.(deg2rad.(angles .- error)) perc_error = er .* 100 ./ μ plot(angles, er .* 100 ./ μ, xlim=(5,70), ylim=(0,200), color=&quot;lightblue&quot;, legend=true, lw=3, label=&quot;Percentual error&quot;) vline!([angles[findfirst(x-&gt;x==minimum(perc_error), perc_error)]], lw=3, label=&quot;Minimum error&quot;) ylabel!(&quot;Δtan(θ)/θ&quot;) xlabel!(&quot;θ [deg]&quot;) title!(&quot;Percentual error&quot;) end So, now we see that the lowest percentual error is obtained when we work in angles near 45°, so we are good to go and we can use the data we measured adding the error in the angle. We now define the new model, where we include an uncertainty in the angle. We propose an uniform prior for the angle centered at 45°, the angle we think the measurement was done. begin @model gravity_angle_uniform(t_final, x_final, θ) = begin # The number of observations. error = 15 angle ~ Uniform(45-error/2, 45 + error/2) g ~ LogNormal(log(4),0.3) μ = g .* (t_final.*t_final./(2 * tan.(deg2rad(angle)))) N = length(t_final) for n in 1:N x_final[n] ~ Normal(μ[n], 10) end end; end begin chain_uniform_angle = sample(gravity_angle_uniform(t_measured, Δx_measured, θ), HMC(ϵ, τ), iterations, progress=false); end; begin histogram(chain_uniform_angle[:g], legend=false, normalized=true) xlabel!(&quot;g_mars&quot;) ylabel!(&quot;Probability&quot;) title!(&quot;Posterior distribution for g, including uncertainty in the angle&quot;) end 6.2.1 Calculating the escape velocity Now that we have calculated the gravity, we are going to calculate the escape velocity. What data do we have until now? we know from the begining that: \\(R_{Earth}\\approxeq 2R_{Mars}\\) \\(g_{Earth}\\approxeq 9.8\\) and we have also computed the distribution of the plausible values of \\(g_{Mars}\\). So, replacing them in the equation of the escape velocity: \\(\\frac{v_{Mars}}{v_{Earth}} =\\sqrt{\\frac{g_{Mars}*R_{Mars}}{g_{Earth}*R_{Earth}}}\\) so, \\(\\frac{v_{Mars}}{11} =\\sqrt{\\frac{g_{Mars}*2*\\cancel{R_{Mars}}}{9.8*\\cancel{R_{Mars}}}} \\qquad \\left[\\frac{km}{s} \\right]\\) \\(v_{Mars} =11 * \\sqrt{\\frac{g_{Mars}}{9.8*2}} \\qquad \\left[\\frac{km}{s} \\right]\\) begin v = 11 .* sqrt.(chain_uniform_angle[:g] ./ (9.8*2)) histogram(v, legend=false, normalized=true) title!(&quot;Escape velocity from Mars&quot;) xlabel!(&quot;Escape Velocity of Mars [km/s]&quot;) ylabel!(&quot;Probability&quot;) end Finally, we obtained the escape velocity scape from Mars. 6.3 Summary In this chapter we had to find the escape velocity from Mars. To solve this problem, we first needed to find the gravity of Mars, so we started with a physical description of the problem and concluded that by measuring the distance and time of a rock throw plus some Bayesian analysis we could infer the gravity of Mars. Then we created a simple probabilistic model, with the prior probability set to a uniform distribution and the likelihood to a normal distribution. We sampled the model and obtained our first posterior probability. We repeated this process two more times, changing the prior distribution of the model for more accurate ones, first with a normal distribution and then with a logarithmic one. Finally, we used the gravity we inferred to calculate the escape velocity from Mars. 6.4 Give us feedback This book is currently in a beta version. We are looking forward to getting feedback and criticism: Submit a GitHub issue here. Mail us to martina.cantaro@lambdaclass.com Thank you! "],["football-simulation.html", "Chapter 7 Football simulation", " Chapter 7 Football simulation "],["basketball-shots.html", "Chapter 8 Basketball shots", " Chapter 8 Basketball shots "],["optimal-pricing.html", "Chapter 9 Optimal pricing", " Chapter 9 Optimal pricing "],["image-classification.html", "Chapter 10 Image classification", " Chapter 10 Image classification "],["ultima-online.html", "Chapter 11 Ultima online", " Chapter 11 Ultima online "],["ultima-continued.html", "Chapter 12 Ultima continued", " Chapter 12 Ultima continued "],["time-series.html", "Chapter 13 Time series", " Chapter 13 Time series "],["epilogue.html", "Epilogue", " Epilogue "]]
